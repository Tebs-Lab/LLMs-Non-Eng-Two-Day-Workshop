# Artificial General Intelligence

Here's a common joke in Computer Science circles: Artificial Intelligence is just a term for anything computers can't do yet. Everything else is just called "software." 

The algorithm that powers Google Maps (A* Graph Search) is an "AI Algorithm" according to the standard textbook -- but no one calls Google Maps "AI" because its "intelligence" isn't anything remotely like human intelligence. 

This research section is devoted to one of the key reasons "AI" is so poorly defined: different notions of what qualifies as "intelligence." Specifically, the difference between "narrow" and "general" intelligence.


## What Are "Narrow" and "General" Intelligence?

* Find a few definitions of "Narrow" artificial intelligence and "General" artificial intelligence.
    * Are the definitions you find consistent?
    * Are they clear and objective?
    * Make your own best attempt to define "Narrow" and "General" Intelligence.

* Some people conceptualize of the distinction between "Narrow" and "General" intelligence as a binary yes/no. Others think of it more as a spectrum.
    * In a few sentences, make your best argument that it's a binary.
    * In a few sentences, make your best argument that it's a spectrum.

* The vast majority of people agree that a "model" on it's own cannot be a general intelligence.
    * Why would the properties of an "agent" be necessary for general intelligence?

* At this point, many LLM based computer systems can pass the original [Turing Test](https://en.wikipedia.org/wiki/Turing_test)... But almost everyone agrees that ChatGPT is not "generally" intelligent. 
    * Can you find any proposals for a modern "Turing Test" -- something that only a "General" intelligence could pass?
    * Can you think of such a test yourself?


## Are LLMs A Path To "General" Intelligence?

* There is massive disagreement about this question, even among highly credentialed experts and luminaries in the field. For example...
    * Yann LeCun, Meta's Chief AI scientist, has consistently argued that LLMs will not yield a "general" intelligence. [Here's an interview where he makes this argument.](https://time.com/6694432/yann-lecun-meta-ai-interview/)
    * Peter Norvig, a long time big wig at Google AI, and the author of the most popular AI textbook, argues that ["Todayâ€™s most advanced AI models have many flaws, but decades from now, they will be recognized as the first true examples of artificial general intelligence."](https://www.noemamag.com/artificial-general-intelligence-is-already-here/) 

* Read the two linked articles (or at least skim them) then answer these questions:
    * What is the most compelling argument that current LLMs might eventually be considered "generally" intelligent, or at least the foundational precursor to general intelligence?
    * What is the most compelling argument that current LLMs will always be considered a form of "narrow" intelligence?


* How are "Zero Shot" and "Few Shot" learning related to "general" and "narrow" intelligence?


## Existential Risks

* What is meant by P(doom)?
    * Define your own personal P(doom).
    * Define your own P(doom) under the assumption that all AI research everywhere came to a halt today.  
    * Define your own P(doom) under the assumption that "general" artificial intelligence is never developed, but increasingly impressive "narrow" systems are developed.
    * Define your own P(doom) under the assumption that "general" artificial intelligence is created within the next 5 years. 

* Much of the conversation around existential risk centers on the concept of "exponential growth" in the abilities of AI systems. Read (or at least skim) the following two articles:
    * [It's Not All Exponential Growth](https://blog.tebs-lab.com/p/not-exponential-growth)
    * [Growth in Artificial Intelligence Is Beyond Exponential](https://www.legacyresearch.com/the-daily-cut/growth-in-artificial-intelligence-is-beyond-exponential/)
    * What is the most compelling argument that growth in AI capabilities will be exponential?
    * What is the most compelling argument that growth in AI capabilities will not be exponential?

## Alternate Paths To General Intelligence

* There are roughly three main camps when it comes to AGI:
    * "Scale is all you need" -- people who believe current methods are enough to get to AGI, and all we need is more data, larger models, and more computational power. 
    * "General intelligence through composition of narrow intelligence" -- people who believe an AGI will most likely be composed of several subsystems each of which is a "narrow" AI system, with some additional mechanisms for coordinating the various subsystems. 
    * "Fundamental paradigm shift" -- people who believe current methods won't yield general intelligence regardless of how large we make them or how many of them we compose. 

* Can you classify the following organizations into one of the three camps, based on their public comments?
    * Meta AI (Led by Yann LeCunn)
    * OpenAI (Led by Sam Altman)
        * Bonus points for former Chief Scientist Ilya Sutskever's viewpoint.
    * Google AI (Led by Jeff Dean, but Sundar Pichai's statements should be considered too)
    * Anthropic (Daniela Amodei, Dario Amodei, Jack Clark, Jared Kaplan)