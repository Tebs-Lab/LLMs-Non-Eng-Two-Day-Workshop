# The Tension Between Progress, Safety, and Societal Benefit

Like most technology, LLMs can be used in a variety of ways. Some uses are clearly beneficial, some are clearly harmful, some have nuanced tradeoffs, some hurt some groups and help other groups.

This research section is focused on identifying the most likely harms that are *currently* being caused by research and deployment of LLMs.

## A Note About Existential Risks

* Many people do in fact worry about existential risk.
* That's probably a good thing to worry about and try to prevent.
* However, this section is about *current* harms, and everyone in this workshop is alive.

## Privacy, Copyright, and Web Scraping

Because huge amounts of data are scrapped from the web to build LLMs, training datasets inevitably include some of the following:

* Private information that was...
    * Published without permission
    * Originally published behind some kind of authentication, but was scrapped anyway (e.g. by hackers)
    * Published accidentally
* Copy protected data.
* Additionally, models have been known to "regurgitate" training data verbatim. (See: https://www.technologyreview.com/2023/02/03/1067786/ai-models-spit-out-photos-of-real-people-and-copyrighted-images/)

With all that in mind:

* With respect to private data, what risks do LLMs pose?
    * What sorts of data might they "regurgitate" that create risks?
    * Consider risks to individuals as well as risk assumed by publishers.
    * What can LLM publishers do to reduce these risks?

* With respect to copy protected data, what risks to LLMs pose?
    * What legal risks might an LLM publisher open themselves to by training with copy protected data?
    * If an LLM plagiarizes someone's work, and an end user publishes it (never knowing where it was plagiarized) how should liability be assigned to the LLM creator vs the LLM user?
    * If an LLM "hallucinates" a citation, should the LLM publisher be held liable for slander / libel? 
        * What specifics of the situation might cause you to lean one way or the other?

* Given the above risks, why don't LLM publishers eliminate or reduce the amount of private and copy protected data they use to train models?

## Mis-and-Dis Information

* Describe at least 3 ways LLMs can be used by propagandists.
    * Can you find any evidence that some of these things have already been done?

* LLMs often "hallucinate," that is, they make stuff up. Given that...
    * Are there any industries or specific purposes where you'd support banning LLMs?

* Spam contributes somewhat to misinformation...
    * Can you find 3 examples of how and why LLMs are being used to generate spam?

* Mis-and-dis information are not new phenomena...
    * Make the best argument you can that LLMs will cause significant change in this area.
    * Now, make the best argument you can that they're not a significant change with respect to mis-and-dis information.

## Energy Use & Climate

* Generative models use an enormous amount of energy...
    * Can you find at least 2 credible sources quantifying that energy use?
    * Based on those sources, about how much energy is used by the LLM industry?

* In addition to energy use, what other resources are being used in large amounts by the AI industry?
    * Hint: https://www.theatlantic.com/technology/archive/2024/03/ai-water-climate-microsoft/677602/

## Regulatory Frameworks

* What existing regulations can you find focused on AI?
    * How to they address the above risks?
* Given your research up to this point, which risks would you most like to see addressed by new regulation?
    * Are there any specific regulations or rules you'd impose if your research group was a legislative body?
* Conversely, which areas would you *least* like to see addressed with regulation?