# Discussion Exercise

You've had some instruction on what LLMs are and how they are trained. Now, your instructor will break in you into smaller groups. In your groups, discuss each of the following questions:

* In real world settings, where do you suppose the training datasets come from?
    * Who compiles them?
    * How are they compiled?
    * To what extent are they audited and cleaned?


* Should system designers allow LLM systems to learn "online" in real time with data from real customers?
    * Why or why not?

* What features are most important when it comes to collecting and curating a training data set?
    * Do pre-training and fine-tuning datasets have the same priorities and considerations?
        * If they are different, how?
        * If they are the same, why?
        * Are there some features that are definitely shared?


* There is genuine tension between collecting huge datasets and collecting high quality datasets... With that in mind:
    * What sorts of compromises are might LLM make along this axis?
    * Are you aware of any competing philosophies on this point?


* What sorts of errors might you expect from statistical models trained this way?
    * What steps might be taken to mitigate those errors?
    * Try to identify at least 3 possible failure patterns.